# Агрегация, обогащение с помощью LLM, фильтрация и визуализация данных о продуктах питания онлайн-магазинов с интеграцией Google Sheets.

Добрый день, уважаемые члены комиссии!

# Введение в предметную область

<!-- Текст, который я озвучу голосом -->

В последние годы все больше людей стремятся к осознанному питанию: следят за калорийностью, балансом белков, жиров и углеводов, избегают определенных ингредиентов. Однако этот процесс сопряжен с рядом трудностей.

Во-первых, это ручной труд. Поиск информации о продуктах, ее сохранение в таблицы или приложения для планирования рациона отнимает много времени. 

Во-вторых, сайты и приложения продуктовых магазинов, где мы совершаем покупки, предоставляют лишь базовые возможности сортировки и фильтрации.
Отсортировать, например, по калориям или по цене за грамм белка или другой сложной формуле -- невозможно. 

В-третьих, многие важные характеристики, такие как уровень содержания лактозы, или соответствие продукта кето-диете, требуют внимательного изучения состава, что сложно и не всегда достоверно.

В-четвертых, сайтов и различных категорий на них - очень много, из-за чего страницы долго грузятся, их изучение занимает много времени.
Хочется иметь простой и удобный интерфейс над всеми этими данными.

Таким образом, существует явная потребность в инструменте, который бы:
- автоматизировал сбор и обработку данных о продуктах питания с нескольких сайтов,
- обогащал их информацией об индивидуальных требованиях к питанию,
- предоставлял быстрый и удобный интерфейс для сортировки, фильтрации и анализа данных,
- интегрировался с сервисами планирования рациона.

<!-- Содержимое слайда -->

- Рост популярности осознанного питания и трекинга рациона.
- **Основные трудности для потребителя:**
  - **Трудоёмкость:** Ручной сбор данных о КБЖУ (калории, белки, жиры, углеводы).
  - **Ограниченный поиск:** Отсутствие у ритейлеров инструментов для поиска продуктов по сложным нутриентным критериям (например, `белок / калории`).
  - **Скрытые характеристики:** Сложность определения соответствия продукта диетам (уровень лактозы, кето и т.д.).
  - **Громоздкость:** Большое количество сайтов и категорий с медленной загрузке страниц приводит к значительным временным затратам на их изучение.
- **Проблема:** Отсутствие единого инструмента, который автоматизирует сбор данных о продуктах питания, обогащает их информацией о диетических требованиях, обеспечивает удобную сортировку и фильтрацию, а также интегрируется с сервисами планирования рациона.

# Проблематика и имеющиеся решения

<!-- Текст, который я озвучу голосом -->

Существующие решения можно разделить на три группы. 
Первая — это сайты самих ритейлеров, как Ozon или ВкусВилл.
Они содержат актуальные данные о ценах и наличии, но их функциональность для поиска крайне ограничена.

Вторая группа — приложения для подсчета калорий, вроде FatSecret, MyFitnessPal.
Их базы данных часто неполны, неточны, ведутся пользователями и, что самое главное, никак не связаны с реальным наличием и ценами в магазинах.

Третья группа — это самодельные решения, например, таблицы в Google Sheets.
Они гибки, но требуют колоссальных ручных усилий по наполнению и поддержанию в актуальном состоянии.

Ни одно из этих решений не предоставляет комплексного подхода, решающего все проблемы потребителя.

<!-- Содержимое слайда -->

- **Аналоги и их недостатки:**
  - **Сайты ритейлеров (Ozon, VkusVill):**
    - `+` Актуальные данные о товарах.
    - `-` Примитивные фильтры, нет сортировки по нутриентам.
  - **Приложения-счетчики калорий (FatSecret, MyFitnessPal):**
    - `+` Удобный интерфейс для ведения дневника.
    - `-` Неполные или неточные данные, нет связи с ценами иналичием в магазинах.
  - **Ручные таблицы (Google Sheets, Excel):**
    - `+` Гибкость.
    - `-` Требуют больших временных затрат на ручное наполнение.

- **Вывод:** Отсутствует единое решение, автоматически объединяющее сильные стороны всех трех групп.

// На этом слайде нужно визуально показать "боль" ручного ведения рациона.

// Слева – изображение типичной таблицы в Google Sheets или Excel для учёта калорий. Колонки: Продукт, Белки, Жиры, Углеводы, Калории, Цена. Ячейки заполнены вручную.

// Справа – скриншоты сайтов Ozon и ВкусВилл, где открыты карточки товаров.

// Между ними стрелки, символизирующие мучительный процесс ручного копирования данных.

// На слайде представлены логотипы существующих решений: Ozon, ВкусВилл, MyFitnessPal, FatSecret.


# Цель и задачи

<!-- Текст, который я озвучу голосом -->

Исходя из описанной проблематики, я поставил перед собой следующую цель. *[Зачитать цель со слайда]*.

Для ее достижения были сформулированы четыре ключевые задачи. *[Зачитать задачи со слайда]*.

Далее последовательно рассказывается о решении каждой из этих задач, технических вызовах, с которыми пришлось столкнуться, и найденных решениях.

<!-- Содержимое слайда -->

- **Цель:** Разработать программный комплекс для 
автоматизированного сбора данных о продуктах питания,
обогащения их дополнительной нутриентной и диетологической информацией,
обеспечения удобной сортировки и фильтрации,
с интеграцией Google Sheets для планирования рациона.

- **Задачи:**
  1.  Разработать модули-парсеры для извлечения данных с сайтов Ozon и VkusVill, способные преодолевать их системы защиты и противодействия скрапингу.
  2.  Создать модуль для автоматического обогащения собранных данных диетологическими атрибутами с использованием больших языковых моделей (LLM).
  3.  Реализовать веб-интерфейс с расширенными возможностями фильтрации, сортировки и анализа данных.
  4.  Интегрировать систему с Google Sheets для обеспечения сквозного цикла планирования рациона.
  5.  Разработать удобный шаблон Google Sheets для планирования рациона с автоматическим расчетом КБЖУ и стоимости рациона.

# Задача 1: Парсинг. Вызовы и решения (Ozon)

<!-- Текст, который я озвучу голосом -->

Задача - реализовать скрапинг, парсинг продуктов питания Ozon Fresh. 

Существующие решения:
- Работающих инструментов для скрапинга продуктов питания (Ozon Fresh) в открытом доступе нет: большинство из них либо устарели, либо не поддерживают актуальные антибот-защиты Озон.
- Классические библиотеки для скрапинга и краулинга типа `Scrapy`, `crawl4ai` не подходят из-за устойчивой антибот-защиты.

Поэтому было принято решение реализовать все самостоятельно.

Проблемы и решения:
1. У Ozon нет публичного API. На все запросы стоит рейтлимит, антибот-челленджи.
- В результате анализа сетевой активности в консоли разработчика были обнаружены внутренние, недокументированные API-эндпоинты, которые используются для получения данных о товарах.
- Однако даже в них не была предоставлена вся нужная информация о продуктах.
- Эндпоинты были разобраны на компоненты: хосты, параметры, синтаксис запроса.
- Был написан скрипт, который перебирает все комбинации компонент составленных декартовым произведением множеств.
- По каждой комбинации был произведен запрос.
- Каждый ответ просматривался на наличие нужных данных.
- Таким образом была составлена карта <нужные_данные_о_товаре> -> <эндпоинт>.
- При этом эти скрытые эндопинты не стоял рейтлимит или антибот-челленджи, что позволило производить запросы через `requests`.
2. Низкое качество данных. Структура и формат предоставляемых данных разнится от товара к товару (прим. кбжу на 100 граммов или на порцию).
- Учтена комплексная логика: товар один или несколько, вес и макронутриенты указаны на одну порцию или на 100 граммов
- Для учета всевозможных форматов написаны регулярные выражения

<!-- Содержимое слайда -->

TODO: написать содержимое слайда на основе вышеизложенного.

// Изобразить комбинаторику эндпоинтов и их ответы.

# Задача 1: Парсинг. Вызовы и решения (VkusVill)

<!-- Текст, который я озвучу голосом -->

ВкусВилл, в отличие от Ozon, оказался проще в плане блокировок, но сложнее в плане извлечения данных. 
У него более щадящий рейт-лимит, что позволило применить параллельный сбор данных. 
Однако, анализ сетевой активности не выявил никаких API-эндпоинтов, даже внутренних.

Единственным путем был полный парсинг HTML-кода каждой страницы товара. 
Главная проблема заключалась в том, что информация о КБЖУ и составе части товаров представлена в виде сплошного текста, без какой-либо четкой структуры.
Где-то пишется "белки 1 г", где-то "1г белки" и так далее. 

Для решения этой задачи пришлось разработать целую систему сложных, вложенных обработок данных с использованием регулярных выражений, которые способны находить и извлекать нужные числовые значения из текстового "хаоса" описаний продуктов. Это был пример классической, но очень кропотливой работы с "грязными" данными.

<!-- Содержимое слайда -->

- **Отличия от Ozon:**
  - `+` Отсутствие рейт-лимитов → возможность быстрого параллельного сбора данных.
  - `-` Полное отсутствие API, даже внутреннего.
- **Проблема:** Данные о КБЖУ и составе представлены в виде неструктурированного текста в общем описании товара.
  - *Пример: "Белки 15 г, жиры 5 г, углеводы 2 г. Энергетическая ценность: 113 ккал."*
  - *Другой пример: "15г белки, 5г жиры, 2г углеводы. 113 ккал - энергетическая ценность."*
  - И так далее.
- **Решение: Regex-магия**
  - Разработана комплексная логика обработки данных, способная находить и валидировать данные КБЖУ в нескольких различных форматах написания.
  - Реализован каскадный парсинг: если не сработала одна регулярка, пробуется следующая.
- **Микровывод:** Создан отказоустойчивый HTML-парсер, способный извлекать структурированную информацию из неструктурированного текста с высокой точностью.


# Задача 1: Парсинг. Проверка доступности по адресу

<!-- Текст, который я озвучу голосом -->

Общей проблемой для обоих магазинов была проверка доступности и актуальной цены товара, так как они зависят от адреса доставки пользователя. Эта информация устанавливается в cookies браузера после выбора адреса.

Решить эту задачу в полностью автоматическом режиме было непросто. Основная сложность заключалась в том, что API-эндпоинты Ozon не поддерживали передачу cookies ни через HTTP-заголовки, ни через JavaScript-инъекции. Был разработан гибридный механизм с использованием Selenium:

При первом запуске скрипт открывает обычный, не-headless браузер с применением selenium-stealth для эмуляции реального пользователя. Пользователь один раз выбирает свой адрес доставки. После этого скрипт извлекает и сохраняет сессионные cookies и параметры сессии в локальный файл.

Все последующие проверки доступности происходят в быстром, headless-режиме с инъекцией этих сохраненных cookies. Для получения актуальной информации о наличии товаров используется прямой парсинг HTML-страниц. Таким образом, удалось реализовать надежный механизм проверки доступности товаров с учетом адреса доставки, минимизировав участие пользователя.

Аналогичное решение было применено для ВкусВилла, но там был использован более легковесный подход с Playwright (`async_playwright`), поскольку у него стоит более щадящая система защиты от скрапинга.

<!-- Содержимое слайда -->

- **Проблема:** Цена и наличие товара зависят от адреса доставки, который хранится в сессионных cookies.
- **Техническая сложность:** 
  - Управление cookies, требующими интерактивного выбора адреса
  - Отсутствие поддержки cookies в API-эндпоинтах Ozon (ни через headers, ни через JS-инъекции)
- **Решение: Playwright + Selenium**
  1. **Авторизация через браузер:**
     - Использование `selenium-stealth` для эмуляции реального браузера
     - Единоразовый запуск видимого браузера для выбора адреса доставки
  2. **Сохранение сессии:**
     - Экстракция cookies и параметров сессии после авторизации
     - Сохранение в локальный файл для последующего использования
  3. **Автоматизация проверок:**
     - Инъекция сохраненных cookies в headless-браузер
     - Прямой парсинг HTML-страниц товаров для получения актуальной информации о наличии
- **Решение для ВкусВилла:**
  - Использование `async_playwright` для эмуляции реального браузера и сохранения cookies в локальный файл. 
  - Дальнейшие действия - аналогично Ozon.
- **Микровывод:** Реализован надежный механизм проверки доступности товаров с учетом адреса доставки через эмуляцию реального браузера и сохранение сессии.


# Задача 2: Обогащение данных с помощью LLM

<!-- Текст, который я озвучу голосом -->

Собранные "сырые" данные, такие как название и состав, не отвечают на сложные вопросы: 
Подходит ли этот йогурт для кето-диеты? 
Много ли в нем клетчатки?
Много ли в нем витамина B12?

Ответить на эти и многие другие вопросы для тысяч товаров - идеальная задача для большой языковой модели. Однако основная техническая сложность заключалась в обеспечении структурированного вывода от LLM, так как языковые модели склонны к галлюцинациям и непредсказуемому формату ответа.

Был разработан скрипт, который для каждого продукта отправляет экспертный-промпт в API Google Gemini. Ключевое решение - использование Pydantic схем для валидации и принуждения к структурированному выводу. Также реализована система retry с экспоненциальной задержкой для обработки rate limiting (15 запросов/минуту для бесплатного API).

Для масштабирования использовались семафоры для контроля concurrent запросов, что позволило эффективно обрабатывать тысячи продуктов. Промпт-инжиниринг с явным указанием JSON-схемы и примеров обеспечивает 96% успешных парсингов ответов.

<!-- Содержимое слайда -->

- **Проблема:** Сырые данные (состав, описание) не позволяют фильтровать по диетологическим признакам (кето, глютен, FODMAP, уровень клетчатки и витаминов, и т.д.).
- **Решение:** Использование LLM (Google Gemini) для семантического анализа и классификации продуктов.
- **Технические вызовы:**
  1. Склонность LLM к галлюцинациям
  2. Непредсказуемый формат ответа
  3. Rate limiting Google Gemini API (15 req/min)
  4. Необходимость обработки ошибок и retry
- **Техническое решение:**
  ```python
  class ComprehensiveProductAnalysisV4(BaseModel):
      fodmap_rating: int = Field(..., ge=1, le=5)
      keto_friendly_rating: int = Field(..., ge=1, le=5)
      gluten_presence_rating: int = Field(..., ge=1, le=5)
      # ... 30+ полей с жестким типированием
  ```
- **Архитектура для масштабирования:**
  ```python
  semaphore = asyncio.Semaphore(3)  # Контроль concurrent запросов
  async with semaphore:
      response = await gemini_model.generate_content(prompt)
  ```
- **Примеры обогащенных полей:**
  - `fodmap_rating` (оценка от 1 до 5)
  - `keto_friendly_rating` (оценка от 1 до 5)
  - `gluten_presence_rating` (оценка от 1 до 5)
  - `health_benefit_tags` (теги: "богат железом", "источник клетчатки")
- **Микровывод:** Создан масштабируемый конвейер обогащения данных с 96% успешных парсингов, который переводит неструктурированный текст в десятки полезных, фильтруемых атрибутов, на порядки повышая ценность собранной информации.

// На слайде можно показать таблицу "до" и "после".
// "До": name, calories, protein, fats, carbs
// "После": ... + fodmap_rating, gluten_presence_rating, keto_friendly_rating, satiety_index, meal_suitability_breakfast_rating, и т.д.

# Задача 3: Веб-интерфейс для анализа

<!-- Текст, который я озвучу голосом -->

Чтобы сделать все эти данные полезными, нужен был удобный интерфейс. Я разработал одностраничное приложение (SPA) на React, Vite, Node.js (Express), Tailwind CSS и многое другое.

Центральным элементом является интерактивная таблица, способная плавно отображать тысячи строк. Она поддерживает мгновенную фильтрацию по любому столбцу, включая обогащенные LLM-данные, сортировку, скрытие и перетаскивание столбцов и многое другое.

Также поддерживаются кастомные формулы для сортировки. Пользователь может ввести в специальное поле математическое выражение, используя названия колонок как переменные. Например, отсортировать по `protein / calories * 100`, чтобы найти самые "белковые" на калорию продукты. Это дает практически безграничную гибкость для анализа.

<!-- Содержимое слайда -->

- **Стек:** React, Vite, Node.js (Express), Tailwind CSS.
- **Ключевой элемент:** Интерактивная таблица с виртуализацией для плавной работы с тысячами строк.
- **Возможности:**
  - Мгновенная фильтрация по любому из ~50 столбцов.
  - Сортировка, скрытие, изменение порядка и ширины столбцов.
  - Запуск проверки доступности товара прямо из интерфейса.
- **Гениальность технического решения: Кастомные формулы сортировки**
  - Реализован парсер, позволяющий пользователю вводить математические выражения, используя заголовки столбцов как переменные.
  - **Пример:** `(protein * 4 + carbohydrates * 4) / price_per_100g`
  - Это превращает таблицу из простого вьюера в мощный аналитический инструмент.
- **<Изображение: Скриншот интерфейса, демонстрирующий фильтры и поле для кастомной формулы>**

// На этом слайде должен быть большой и красивый скриншот самого веб-интерфейса, демонстрирующий таблицу с фильтрами, картинками товаров и меню действий.

# Задача 4: Интеграция с Google Sheets

<!-- Текст, который я озвучу голосом -->

Последним шагом было замкнуть цикл "поиск-анализ-планирование". Для этого я реализовал интеграцию с Google Sheets.

Из веб-интерфейса любой понравившийся продукт можно одним кликом отправить в свою гугл-таблицу. Для этого на бэкенде используется Google Sheets API с аутентификацией через сервисный аккаунт, что избавляет пользователя от необходимости проходить OAuth-авторизацию.

<!-- Содержимое слайда -->

- **Цель интеграции:** Обеспечить удобный перенос найденных продуктов в личный план питания.
- **Механизм:**
  - **Бэкенд:** Node.js (Express) проксирует запросы к Google Sheets API.
  - **Аутентификация:** Используется сервисный аккаунт (`credentials.json`), что делает процесс бесшовным для пользователя (не требует всплывающих окон Google).
  - **Фронтенд:** Кнопка "Экспорт" в меню каждой строки товара.
- **Микровывод:** Реализован сквозной пользовательский сценарий: от поиска уникального продукта до его бесшовного включения в персональный план питания.

// На слайде показать скриншот, где открыто модальное окно экспорта в Google Sheets в веб-интерфейсе, и рядом – сама таблица Google Sheets с уже добавленной строкой.

# Задача 5: Шаблон для планирования рациона

<!-- Текст, который я озвучу голосом -->
Собранная и отфильтрованная база данных — это хорошо, но как её использовать для конечной цели, планирования рациона?
Вместе с этим я разработал специальный шаблон таблицы, который использует добавленные продукты как базу данных.
С его помощью можно удобно планировать рацион на неделю, а КБЖУ и стоимость каждого приема пищи и каждого дня будут рассчитываться автоматически с помощью стандартных формул Google Sheets.

Он в полной мере повторяет функциональность классических инструментов планирования рациона, но при этом бесплатен, работает на всех устройствах.

<!-- Содержимое слайда -->

**Решение: Разработка продвинутого шаблона Google Sheets.**
*   **Источник данных:** Шаблон использует лист "База продуктов" в качестве источника данных. Этот лист пополняется одним кликом из веб-интерфейса.
*   **Планировщик:** На основном листе "План на неделю" пользователь может выбрать продукты для каждого приема пищи из выпадающих списков, которые ссылаются на базу.
*   **Автоматические расчеты:** Формулы (`SUMIF`, `VLOOKUP`) автоматически подсчитывают итоговое количество калорий, белков, жиров, углеводов и общую стоимость рациона за каждый день и за всю неделю.
- **Разработанный шаблон для планирования:**
  - Лист «База продуктов», куда попадают товары из веб-интерфейса.
  - Листы «План на неделю», «Меню на день», которые с помощью `VLOOKUP` и других формул автоматически рассчитывают КБЖУ и стоимость рациона.


// Показать скриншот шаблона Google Sheets. Видно несколько вкладок: "База продуктов", "План на неделю". На вкладке "План на неделю" – таблица с днями недели и приемами пищи, куда из выпадающего списка добавляются продукты из "Базы". Сбоку – итоговые суммы по КБЖУ и цене за день.

# Архитектура системы и интеграция компонентов

<!-- Текст, который я озвучу голосом -->

Общая архитектура системы построена по принципу модульности с четким разделением ответственности между компонентами.

Ключевое архитектурное решение - использование CSV как промежуточного формата данных между всеми компонентами. Это обеспечивает loose coupling и возможность замены любого модуля без влияния на остальные.

Система миграции данных автоматизирует процесс обновления: 

```python
# scripts/data_migration.py
def migrate_data(mode='update'):
    for scraper_dir in ['ozon-scraper', 'vkusvill-scraper']:
        for category_csv in find_detailed_csvs(scraper_dir):
            target_path = f"csv/{shop_name}/{category_name}.csv"
            merge_or_update(category_csv, target_path, mode)
```

Обеспечена устойчивость к сбоям через систему backup'ов и rollback механизмы.

Разработан конвейер обработки данных: Парсинг → Миграция → Обогащение → Веб-интерфейс, где каждый этап может работать независимо.

Техническая инновация - Split Tunneling для одновременной работы с российскими сайтами (требуют российский IP) и Google Sheets API (требует зарубежный IP).

Микровывод: модульная архитектура обеспечивает масштабируемость, отказоустойчивость и возможность независимого развития компонентов.

<!-- Содержимое слайда -->

**Архитектурные принципы:**
- Модульность с четким разделением ответственности
- Loose coupling через CSV как промежуточный формат
- Независимость компонентов друг от друга

**Конвейер обработки данных:**
```
Парсинг → Миграция → Обогащение → Веб-интерфейс
   ↓         ↓           ↓            ↓
 CSV      CSV       CSV+AI       React UI
```

**Система миграции данных:**
```python
def migrate_data(mode='update'):
    for scraper_dir in ['ozon-scraper', 'vkusvill-scraper']:
        for category_csv in find_detailed_csvs(scraper_dir):
            target_path = f"csv/{shop_name}/{category_name}.csv"
            merge_or_update(category_csv, target_path, mode)
```

**Обеспечение отказоустойчивости:**
- Автоматические backup'ы перед обновлениями
- Rollback механизмы при сбоях
- Система логирования и мониторинга

**Техническая инновация - Split Tunneling:**
- Российские сайты (Ozon, ВкусВилл) → прямое соединение
- Google Sheets API → VPN туннель
- Решение через NekoRay с настройкой прямых маршрутов

**Результат:** Масштабируемая архитектура с независимыми компонентами

# Выводы и результаты

<!-- Текст, который я озвучу голосом -->

В результате проделанной работы цель проекта была полностью достигнута. Разработанный программный комплекс решает поставленные задачи.

Были решены нетривиальные технические задачи: создан гибридный парсер, способный обходить анти-бот системы за счет реверс-инжиниринга; разработан сложный regex-парсер для извлечения данных из хаотичного HTML.

Внедрен конвейер LLM-обогащения, превращающий текст в структурированные данные. Создан уникальный интерфейс с кастомными формулами сортировки. И все это связано сквозной интеграцией с Google Sheets. В дополнение к этому был разработан шаблон таблицы для планирования рациона, который в полной мере повторяет функциональность классических инструментов планирования рациона и дополняет ее автоматическим расчетом стоимости рациона на основе добавленных продуктов.

Главный практический результат работы системы — это возможность находить действительно уникальные продукты, о существовании которых раньше нельзя было и догадаться. Например, с помощью формулы сортировки по калорийности я нашел творог с 55 ккал на 100 грамм. Весь процесс выбора продуктов и планирования рациона, который раньше занимал часы, теперь занимает минуты.

Спасибо за внимание! Готов ответить на ваши вопросы.

<!-- Содержимое слайда -->

- **Цель работы достигнута:** Разработан и внедрен программный комплекс, автоматизирующий поиск, анализ и планирование рациона.
- **Ключевые технические достижения:**
  1.  **Парсинг:** Решена задача сбора данных с защищенных сайтов путем комбинации реверс-инжиниринга внутреннего API, эмуляции браузера (`playwright-stealth`) и сложных regex-парсеров.
  2.  **Обогащение данных:** Внедрен LLM-конвейер, добавивший ~30 диетологических атрибутов к каждому товару.
  3.  **Анализ:** Создан интерактивный веб-интерфейс с уникальной функцией сортировки по пользовательским математическим формулам.
  4.  **Интеграция:** Реализован бесшовный экспорт данных в Google Sheets для планирования.
- **Практический результат:**
  - **Эффективность:** Процесс поиска оптимальных продуктов и ведения дневника питания ускорен на порядок.
  - **Открытия:** Система позволила обнаружить уникальные продукты с выдающимися характеристиками (например, творог 55 ккал/100г, высокобелковые йогурты с низкой ценой), недоступные для поиска стандартными средствами. 